{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e043b6-bc29-4cac-918b-124116e6615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: faiss-cpu in /opt/conda/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9688f6c5-99c8-46a2-8991-a9c19e2dd894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-12-04 09:35:51.848691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize FAISS index\n",
    "embedding_dim = 384  # Adjust based on the embedding model\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "metadata = []  # To store metadata for each passage\n",
    "\n",
    "# Load embedding model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Generate embeddings for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Directory containing JSON files\n",
    "json_dir = \"./json\"  # Update this path based on your folder structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02764fb5-b802-4766-89b0-5f461ffa65f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-JSON file: ideas.qmd\n",
      "Indexing completed!\n"
     ]
    }
   ],
   "source": [
    "# Parse JSON files and add embeddings to FAISS index\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        print(f\"Skipping non-JSON file: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(json_dir, file_name)\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Process the JSON content\n",
    "        for opinion in data.get(\"casebody\", {}).get(\"opinions\", []):\n",
    "            text = opinion.get(\"text\", \"\")\n",
    "            if not text:\n",
    "                continue  # Skip if no text found\n",
    "            \n",
    "            # Split into smaller passages if too long\n",
    "            passages = [text[i:i + 300] for i in range(0, len(text), 300)]\n",
    "            for passage in passages:\n",
    "                embedding = embed_text(passage)\n",
    "                index.add(embedding)\n",
    "                metadata.append({\"file\": file_name, \"text\": passage})\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "# Save FAISS index and metadata\n",
    "faiss.write_index(index, \"legal_cases_index.faiss\")\n",
    "with open(\"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "print(\"Indexing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2848fcc8-4713-4396-8a22-7b08452a4fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages indexed: 3777\n",
      "Sample metadata entry: {'file': '0001-01.json', 'text': 'DAWSON, District Judge.\\nPetitioner, by his guardian, ad litem, sets forth that he is unlawfully restrained of his liberty by Lieutenant Commander J. S. Newell, naval officer in charge at this station, and in command of the United States steamer and man-of-war Pinta. He states that he was enlisted in'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load and inspect metadata\n",
    "with open(\"metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "    print(f\"Number of passages indexed: {len(metadata)}\")\n",
    "    print(\"Sample metadata entry:\", metadata[0] if metadata else \"No entries found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
